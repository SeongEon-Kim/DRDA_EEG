{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import FC, Discriminator, Loss_dis\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data path check\n",
      "s01.mat s15.mat s29.mat s28.mat s14.mat s16.mat s02.mat s03.mat s17.mat s13.mat s07.mat s06.mat s12.mat s04.mat s10.mat s11.mat s05.mat s20.mat s08.mat s09.mat s21.mat s23.mat s22.mat s32.mat s26.mat s27.mat s19.mat s25.mat s31.mat s30.mat s24.mat s18.mat "
     ]
    }
   ],
   "source": [
    "\n",
    "## data load\n",
    "\n",
    "path = r'../DEAP/data_preprocessed_matlab/'  # 경로는 저장 파일 경로\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "print(\"data path check\")\n",
    "for i in file_list:    # 확인\n",
    "    print(i, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read data: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(file_list, desc=\"read data\"): \n",
    "    mat_file = io.loadmat(path+i)\n",
    "    data = mat_file['data']\n",
    "    labels = np.array(mat_file['labels'])\n",
    "    val = labels.T[0].round().astype(np.int8)\n",
    "    aro = labels.T[1].round().astype(np.int8)\n",
    "    \n",
    "    if(i==\"s01.mat\"): \n",
    "        Data = data\n",
    "        VAL = val\n",
    "        ARO = aro\n",
    "        continue\n",
    "        \n",
    "    Data = np.concatenate((Data ,data),axis=0)   # 밑으로 쌓아서 하나로 만듬\n",
    "    VAL = np.concatenate((VAL ,val),axis=0)\n",
    "    ARO = np.concatenate((ARO ,aro),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "1280\n"
     ]
    }
   ],
   "source": [
    "print(len(VAL))\n",
    "print(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess channel: 100%|██████████| 1280/1280 [00:00<00:00, 100267.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# eeg preprocessing\n",
    "\n",
    "eeg_data = []\n",
    "peripheral_data = []\n",
    "\n",
    "for i in tqdm(range(len(Data)), desc=\"preprocess channel\"):\n",
    "    for j in range (40): \n",
    "        if(j < 32): # get channels 1 to 32\n",
    "            eeg_data.append(Data[i][j])\n",
    "        else:\n",
    "            peripheral_data.append(Data[i][j])\n",
    "\n",
    "# set data type, shape\n",
    "eeg_data = np.reshape(eeg_data, (len(Data),1,32, 8064))\n",
    "eeg_data=eeg_data.astype('float32')\n",
    "eeg_data32 = torch.from_numpy(eeg_data)\n",
    "VAL = (torch.from_numpy(VAL)).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data split\n",
      "make data loader\n",
      "device:  mps:0\n"
     ]
    }
   ],
   "source": [
    "#data 40 x 40 x 8064 video/trial x channel x data\n",
    "#labels 40 x 4 video/trial x label (valence, arousal, dominance, liking)\n",
    "#32명 -> 4명 / 14명 14명\n",
    "\n",
    "# data split\n",
    "print(\"data split\")\n",
    "train_data, val_data,train_label, val_label = train_test_split(eeg_data32, VAL, test_size=0.125)\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.5)\n",
    "\n",
    "# make data loader\n",
    "print(\"make data loader\")\n",
    "target_dataset = TensorDataset(x_train, y_train)\n",
    "source_dataset = TensorDataset(x_test, y_test)\n",
    "val_dataset = TensorDataset(val_data, val_label)\n",
    "target_dataloader = DataLoader(target_dataset, 64, shuffle=True)\n",
    "source_dataloader = DataLoader(source_dataset, 64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, 64, shuffle=True)\n",
    "\n",
    "# cuda\n",
    "device = torch.device(\"mps:0\")\n",
    "print(\"device: \", device)\n",
    "\n",
    "#model\n",
    "dis = Discriminator(15960).to(device)\n",
    "fc = FC(32).to(device)\n",
    "\n",
    "#optim\n",
    "optimizer_dis = optim.SGD(dis.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_cls = optim.SGD(fc.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_loss(device, features, labels, alpha, num_classes, name):\n",
    "\n",
    "    len_features = features.size(1)\n",
    "\n",
    "    centers = torch.zeros(num_classes, len_features).float().to(device)\n",
    "    #if torch.cuda.is_available():\n",
    "    #    centers = centers.cuda()\n",
    "\n",
    "    labels = labels.view(-1, 1).long()\n",
    "    centers_batch = centers.index_select(0, labels.squeeze()) # gather -> index_select\n",
    "    diff = centers_batch - features\n",
    "    unique_label, unique_idx, unique_count = torch.unique(labels, sorted=True, return_inverse=True, return_counts=True)\n",
    "    #appear_times = unique_count.index_select(0, unique_idx)   # 일단 무시 \n",
    "    #appear_times = appear_times.view(-1, 1)\n",
    "\n",
    "    #diff = diff / (1 + appear_times.float())\n",
    "    diff = alpha * diff\n",
    "\n",
    "    centers_update_op = torch.zeros(num_classes, len_features).to(device).scatter_add_(0, labels.repeat(1, len_features), diff)\n",
    "    # scatter_sub -> scatter_add_\n",
    "\n",
    "    loss = torch.mean(torch.abs(features - centers_batch))\n",
    "\n",
    "    return loss, centers, centers_batch, diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : epoch\n",
      "batch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_unique2' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb 셀 8\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m g_loss_ce_s \u001b[39m=\u001b[39m criterion(pred_s, y_source\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# center loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m g_loss_center, centers, centers_batch, diff \u001b[39m=\u001b[39m get_center_loss(device, feat_t, y_target\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, alpha\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, num_classes\u001b[39m=\u001b[39;49m\u001b[39m9\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcenters\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m#center = torch.mean(feat_t)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m#g_loss_center = torch.mean(torch.square(feat_t - centers))/2\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m g_loss \u001b[39m=\u001b[39m g_loss_adv \u001b[39m+\u001b[39m g_loss_ce_s \u001b[39m+\u001b[39m g_loss_ce_t \u001b[39m+\u001b[39m g_loss_center\n",
      "\u001b[1;32m/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb 셀 8\u001b[0m in \u001b[0;36mget_center_loss\u001b[0;34m(device, features, labels, alpha, num_classes, name)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m centers_batch \u001b[39m=\u001b[39m centers\u001b[39m.\u001b[39mindex_select(\u001b[39m0\u001b[39m, labels\u001b[39m.\u001b[39msqueeze()) \u001b[39m# gather -> index_select\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m diff \u001b[39m=\u001b[39m centers_batch \u001b[39m-\u001b[39m features\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m unique_label, unique_idx, unique_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49munique(labels, \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_inverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_counts\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#appear_times = unique_count.index_select(0, unique_idx)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#appear_times = appear_times.view(-1, 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#diff = diff / (1 + appear_times.float())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m diff \u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39m diff\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPU_base/lib/python3.9/site-packages/torch/_jit_internal.py:483\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m     dispatch_flag \u001b[39m=\u001b[39m args[arg_index]\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m dispatch_flag:\n\u001b[0;32m--> 483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPU_base/lib/python3.9/site-packages/torch/_jit_internal.py:483\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m     dispatch_flag \u001b[39m=\u001b[39m args[arg_index]\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m dispatch_flag:\n\u001b[0;32m--> 483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPU_base/lib/python3.9/site-packages/torch/functional.py:791\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    783\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39munique_dim(\n\u001b[1;32m    784\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    785\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m         return_counts\u001b[39m=\u001b[39mreturn_counts,\n\u001b[1;32m    789\u001b[0m     )\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 791\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_unique2(\n\u001b[1;32m    792\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    793\u001b[0m         \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m,\n\u001b[1;32m    794\u001b[0m         return_inverse\u001b[39m=\u001b[39;49mreturn_inverse,\n\u001b[1;32m    795\u001b[0m         return_counts\u001b[39m=\u001b[39;49mreturn_counts,\n\u001b[1;32m    796\u001b[0m     )\n\u001b[1;32m    797\u001b[0m \u001b[39mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_unique2' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "#train\n",
    "# At each iteration, we first update the parameters of the domain discriminator,\\\n",
    "# fix the feature extractor and classifier, and then fix the\n",
    "#  domain discriminator and update the parameters of both the\n",
    "#   feature extractor and classifier.\n",
    "\n",
    "g_loss_log = []\n",
    "d_loss_log = []\n",
    "accuracy_s = []\n",
    "accuracy_d = []\n",
    "accuracy_val = []\n",
    "\n",
    "best_loss = 10000000\n",
    "limit_epoch = 9\n",
    "limit_check = 0\n",
    "val_loss = 0\n",
    "nb_epochs = 3\n",
    "for epoch in tqdm(range(nb_epochs+1)):\n",
    "    temp_gloss = 0\n",
    "    temp_dloss = 0\n",
    "    temp_accuracy_d = 0\n",
    "    temp_accuracy_s = 0\n",
    "    temp_accuracy_val = 0\n",
    "\n",
    "    print(epoch, \": epoch\")\n",
    "\n",
    "    temp = 0.0 #batch count\n",
    "    fc.train()\n",
    "    for i, (target, source) in enumerate(zip(target_dataloader, source_dataloader)):\n",
    "        temp += 1.0\n",
    "        print(\"batch: \" , i)\n",
    "\n",
    "        #print(i, target[0].shape, target[1].shape)\n",
    "        #print(source[0].shape, source[1].shape)\n",
    "        x_target, y_target = target[0], target[1]\n",
    "        x_source, y_source = source[0], source[1]\n",
    "\n",
    "        x_target = x_target.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "        x_source = x_source.to(device)\n",
    "        y_source = y_source.to(device)\n",
    "        \n",
    "        feat_t, pred_t = fc.forward(x_target)\n",
    "        feat_s, pred_s = fc.forward(x_source)\n",
    "        \n",
    "        dc_t = dis(feat_t)\n",
    "        dc_s = dis(feat_s)\n",
    "        temp_dc_s = dc_s.clone().detach()\n",
    "\n",
    "        #discriminators loss\n",
    "        dis_loss = Loss_dis(dc_t, dc_s)\n",
    "        \n",
    "        for p in fc.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in dis.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        optimizer_dis.zero_grad()\n",
    "        dis_loss.backward(retain_graph=True)\n",
    "        optimizer_dis.step()\n",
    "    \n",
    "        for p in fc.parameters():\n",
    "            p.requires_grad = True   \n",
    "        for p in dis.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        #global loss\n",
    "        g_loss_adv = torch.mean(torch.square(temp_dc_s-1))/2\n",
    "        g_loss_ce_t = criterion(pred_t, y_target-1)\n",
    "        g_loss_ce_s = criterion(pred_s, y_source-1)\n",
    "        \n",
    "        # center loss\n",
    "        g_loss_center, centers, centers_batch, diff = get_center_loss(device, feat_t, y_target-1, alpha=0.5, num_classes=9, name='centers')\n",
    "        #center = torch.mean(feat_t)\n",
    "        #g_loss_center = torch.mean(torch.square(feat_t - centers))/2\n",
    "        g_loss = g_loss_adv + g_loss_ce_s + g_loss_ce_t + g_loss_center\n",
    "         \n",
    "        optimizer_cls.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_cls.step()\n",
    "        \n",
    "        temp_gloss += g_loss\n",
    "        temp_dloss += dis_loss\n",
    "        print((torch.argmax(pred_t,1)+1)[:10])\n",
    "        print(y_target[:10])\n",
    "\n",
    "        temp_accuracy_s += ((torch.argmax(pred_t,1)+1)== y_target).to(torch.float).mean()\n",
    "        temp_accuracy_d += ((torch.argmax(pred_s,1)+1)== y_source).to(torch.float).mean()\n",
    "    \n",
    "    print(\"\\ngloss\", temp_gloss/temp)\n",
    "    print(\"dloss\", temp_dloss/temp)\n",
    "    print(\"acc_d\", temp_accuracy_d/temp)\n",
    "    print(\"acc_s\", temp_accuracy_s/temp)\n",
    "    \n",
    "    g_loss_log.append(temp_gloss/temp)\n",
    "    d_loss_log.append(temp_dloss/temp)\n",
    "    accuracy_d.append(temp_accuracy_d/temp)\n",
    "    accuracy_s.append(temp_accuracy_s/temp)\n",
    "    \n",
    "    fc.eval()\n",
    "    val_loss = 0\n",
    "    batch = 0\n",
    "    \n",
    "    for x_val, y_val in val_dataloader:\n",
    "        x_val = x_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "        _, y_pred = fc.forward(x_val)\n",
    "        loss = criterion(y_pred, y_val-1)\n",
    "        val_loss += loss.item()\n",
    "        temp_accuracy_val += ((torch.argmax(y_pred,1)+1)== y_val).to(torch.float).mean()\n",
    "        batch = batch + 1\n",
    "    if val_loss > best_loss:\n",
    "        limit_check += 1\n",
    "        if(limit_check >= limit_epoch):\n",
    "            break\n",
    "    else:\n",
    "        best_loss = val_loss\n",
    "        limit_check = 0\n",
    "        #python PROJECT\\drda\\drda_torch\\main.py\n",
    "    print(\"acc_val\", temp_accuracy_val/batch)\n",
    "    accuracy_val.append(temp_accuracy_val/batch)\n",
    "\n",
    "print(g_loss_log)\n",
    "print(d_loss_log)\n",
    "print(accuracy_d)\n",
    "print(accuracy_s)\n",
    "print(\"val_loss \", val_loss)\n",
    "print(accuracy_val)\n",
    "\n",
    "torch.save(fc, './fc.pt')\n",
    "torch.save(dis, './dis.pt')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YourFavoriteNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# 또는, 다른 장치와 마찬가지로 MPS로 이동할 수도 있습니다.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m YourFavoriteNet()  \u001b[39m# 어떤 모델의 객체를 생성한 뒤,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mto(mps_device)       \u001b[39m# MPS 장치로 이동합니다.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gimseong-eon/Desktop/DRDA/DRDA_torch-main/main.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# 이제 모델과 텐서를 호출하면 GPU에서 연산이 이뤄집니다.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YourFavoriteNet' is not defined"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# mps_device = torch.device(\"mps\")\n",
    "\n",
    "# # MPS 장치에 바로 tensor를 생성합니다.\n",
    "# x = torch.ones(5, device=mps_device)\n",
    "# # 또는\n",
    "# x = torch.ones(5, device=\"mps\")\n",
    "\n",
    "# # GPU 상에서 연산을 진행합니다.\n",
    "# y = x * 2\n",
    "\n",
    "# # 또는, 다른 장치와 마찬가지로 MPS로 이동할 수도 있습니다.\n",
    "# model = YourFavoriteNet()  # 어떤 모델의 객체를 생성한 뒤,\n",
    "# model.to(mps_device)       # MPS 장치로 이동합니다.\n",
    "\n",
    "# # 이제 모델과 텐서를 호출하면 GPU에서 연산이 이뤄집니다.\n",
    "# pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('GPU_base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c7976fc9c27cce57a91a4093c4fd52c90bb73b0f6b779a22b5201159a1e33a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
